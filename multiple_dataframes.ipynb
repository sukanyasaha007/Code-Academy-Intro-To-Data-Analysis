{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Multiple DataFrames\n",
    "\n",
    "In order to efficiently store data, we often spread related information across multiple tables.\n",
    "\n",
    "For instance, imagine that we own an e-commerce business and we want to track the products that have been ordered from our website.\n",
    "\n",
    "We could have one table with all of the following information:\n",
    "\n",
    "* order_id\n",
    "* customer_id\n",
    "* customer_name\n",
    "* customer_address\n",
    "* customer_phone_number\n",
    "* product_id\n",
    "* product_description\n",
    "* product_price\n",
    "* quantity\n",
    "* timestamp\n",
    "\n",
    "However, a lot of this information would be repeated. If the same customer makes multiple orders, that customer's name, address, and phone number will be reported multiple times. If the same product is ordered by multiple customers, then the product price and description will be repeated. This will make our orders table big and unmanageable.\n",
    "\n",
    "So instead, we can split our data into three tables:\n",
    "\n",
    "\n",
    "1. orders would contain just the information necessary to describe what was ordered:\n",
    "   * product_id\n",
    "   * customer_id\n",
    "   * quantity\n",
    "   * timestamp\n",
    "\n",
    "\n",
    "2. products would contain the information to describe each product:\n",
    "   * product_id\n",
    "   * product_description\n",
    "   * product_price\n",
    "\n",
    "\n",
    "3. customers would contain the information for each customer:\n",
    "   * customer_id\n",
    "   * customer_name\n",
    "   * customer_address\n",
    "   * customer_phone_number\n",
    "\n",
    "In this lesson, we will learn the Pandas commands that help us work with data stored in multiple tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  customer_id  product_id  quantity   timestamp\n",
      "0         1            2           3         1  2017-01-01\n",
      "1         2            2           2         3  2017-01-01\n",
      "2         3            3           1         1  2017-01-01\n",
      "3         4            3           2         2  2017-02-01\n",
      "4         5            3           3         3  2017-02-01\n",
      "5         6            1           4         2  2017-03-01\n",
      "6         7            1           1         1  2017-02-02\n",
      "7         8            1           4         1  2017-02-02 \n",
      "\n",
      "   product_id         description  price\n",
      "0           1      thing-a-ma-jig      5\n",
      "1           2  whatcha-ma-call-it     10\n",
      "2           3          doo-hickey      7\n",
      "3           4               gizmo      3 \n",
      "\n",
      "   customer_id customer_name        address  phone_number\n",
      "0            1    John Smith   123 Main St.  212-123-4567\n",
      "1            2      Jane Doe  456 Park Ave.  949-867-5309\n",
      "2            3     Joe Schmo   798 Broadway  112-358-1321 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "orders = pd.read_csv('orders.csv')\n",
    "print(orders,'\\n')\n",
    "products = pd.read_csv('products.csv')\n",
    "print(products,'\\n')\n",
    "customers = pd.read_csv('customers.csv')\n",
    "print(customers,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inner Merge\n",
    "\n",
    "Suppose we have the following three tables that describe our eCommerce business:\n",
    "\n",
    "* orders — a table with information on each transaction:\n",
    "\n",
    "<table>\n",
    "    <tr><th>order_id</th><th>customer_id</th><th>product_id</th><th>quantity</th><th>timestamp</th></tr>\n",
    "    <tr><td>1</td><td>2</td><td>3</td><td>1</td><td>2017-01-01</td></tr>\n",
    "    <tr><td>2</td><td>2</td><td>2</td><td>3</td><td>2017-01-01</td></tr>\n",
    "    <tr><td>3</td><td>3</td><td>1</td><td>1</td><td>2017-01-01</td></tr>\n",
    "    <tr><td>4</td><td>3</td><td>2</td><td>2</td><td>2017-02-01</td></tr>\n",
    "    <tr><td>5</td><td>3</td><td>3</td><td>3</td><td>2017-02-01</td></tr>\n",
    "    <tr><td>6</td><td>1</td><td>4</td><td>2</td><td>2017-03-01</td></tr>\n",
    "    <tr><td>7</td><td>1</td><td>1</td><td>1</td><td>2017-02-02</td></tr>\n",
    "    <tr><td>8</td><td>1</td><td>4</td><td>1</td><td>2017-02-02</td></tr>\n",
    "</table>\n",
    "\n",
    "* products — a table with product IDs, descriptions, and prices:\n",
    "\n",
    "<table>\n",
    "    <tr><th>product_id</th><th>description</th><th>price</th></tr>\n",
    "    <tr><td>1</td><td>thing-a-ma-jig</td><td>5</td></tr>\n",
    "    <tr><td>2</td><td>whatcha-ma-call-it</td><td>10</td></tr>\n",
    "    <tr><td>3</td><td>doo-hickey</td><td>7</td></tr>\n",
    "    <tr><td>4</td><td>gizmo</td><td>3</td></tr>\n",
    "</table>\n",
    "\n",
    "* customers — a table with customer names and contact information:\n",
    "\n",
    "<table>\n",
    "    <tr><th>customer_id</th><th>customer_name</th><th>address</th><th>phone_number</th></tr>\n",
    "    <tr><th>1</th><td>John Smith</td><td>123 Main St.</td><td>212-123-4567</td></tr>\n",
    "    <tr><th>2</th><td>Jane Doe</td><td>456 Park Ave.</td><td>949-867-5309</td></tr>\n",
    "    <tr><th>3</th><td>Joe Schmo</td><td>798 Broadway</td><td>112-358-1321</td></tr>\n",
    "</table>\n",
    "\n",
    "If we just look at the orders table, we can't really tell what's happened in each order. However, if we refer to the other tables, we can get a more complete picture.\n",
    "\n",
    "Let's examine the order with an order_id of 1. It was purchased by Customer 2. To find out the customer's name, we look at the customers table and look for the item with a customer_id value of 2. We can see that Customer 2's name is Jane Doe and that she lives at 456 Park Ave.\n",
    "\n",
    "Doing this kind of matching is called merging two DataFrames.\n",
    "\n",
    "The .merge method looks for columns that are common between two DataFrames and then looks for rows where those column's values are the same. It then combines the matching rows into a single row in a new table.\n",
    "\n",
    "We can call the pd.merge method with two tables like this:\n",
    "\n",
    "`new_df = pd.merge(orders, customers)`\n",
    "\n",
    "This will match up all of the customer information to the orders that each customer made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are an analyst Cool T-Shirts Inc. You are going to help them analyze some of their sales data.\n",
    "\n",
    "There are two DataFrames defined in the file script.py:\n",
    "\n",
    "* sales contains the monthly revenue for Cool T-Shirts Inc. It has two columns: month and revenue.\n",
    "* targets contains the goals for monthly revenue for each month. It has two columns: month and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  revenue\n",
      "0   January      300\n",
      "1  February      290\n",
      "2     March      310\n",
      "3     April      325\n",
      "4       May      475\n",
      "5      June      495 \n",
      "\n",
      "      month  target\n",
      "0   January     310\n",
      "1  February     270\n",
      "2     March     300\n",
      "3     April     350\n",
      "4       May     475\n",
      "5      June     500\n"
     ]
    }
   ],
   "source": [
    "sales = pd.read_csv('sales.csv')\n",
    "print(sales,'\\n')\n",
    "targets = pd.read_csv('targets.csv')\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new DataFrame sales_vs_targets which contains the merge of sales and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  revenue  target\n",
      "0   January      300     310\n",
      "1  February      290     270\n",
      "2     March      310     300\n",
      "3     April      325     350\n",
      "4       May      475     475\n",
      "5      June      495     500\n"
     ]
    }
   ],
   "source": [
    "sales_vs_targets = pd.merge(sales, targets)\n",
    "print(sales_vs_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool T-Shirts Inc. wants to know the months when they crushed their targets.\n",
    "\n",
    "Select the rows from sales_vs_targets where revenue is greater than target. Save these rows to the variable crushing_it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>revenue</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>February</td>\n",
       "      <td>290</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>310</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      month  revenue  target\n",
       "1  February      290     270\n",
       "2     March      310     300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crushing_it = sales_vs_targets[sales_vs_targets.revenue > sales_vs_targets.target]\n",
    "crushing_it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using pd.merge, each DataFrame has its own merge method. For instance, if you wanted to merge orders with customers, you could use:\n",
    "\n",
    "`new_df = orders.merge(customers)`\n",
    "\n",
    "This produces the same DataFrame as if we had called pd.merge(orders, customers).\n",
    "\n",
    "We generally use this when we are joining more than two DataFrames together because we can \"chain\" the commands. The following command would merge orders to customers, and then the resulting DataFrame to products:\n",
    "\n",
    "`big_df = orders.merge(customers).merge(products)`\n",
    "\n",
    "We have some more data from Cool T-Shirts Inc. The number of men's and women's t-shirts sold per month is in a file called men_women_sales.csv. Load this data into a DataFrame called men_women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_women = pd.read_csv('men_women_sales.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all three DataFrames (sales, targets, and men_women) into one big DataFrame called all_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  revenue  target  men  women\n",
      "0   January      300     310   30     35\n",
      "1  February      290     270   29     35\n",
      "2     March      310     300   31     29\n",
      "3     April      325     350   32     28\n",
      "4       May      475     475   47     50\n",
      "5      June      495     500   49     45\n"
     ]
    }
   ],
   "source": [
    "all_data = sales.merge(targets).merge(men_women)\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool T-Shirts Inc. thinks that they have more revenue in months where they sell more women's t-shirts.\n",
    "\n",
    "Select the rows of all_data where:\n",
    "\n",
    "    revenue is greater than target\n",
    "\n",
    "AND\n",
    "\n",
    "    women is greater than men\n",
    "\n",
    "Save your answer to the variable results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  revenue  target  men  women\n",
      "1  February      290     270   29     35\n"
     ]
    }
   ],
   "source": [
    "results = all_data[(all_data.revenue > all_data.target) & (all_data.women > all_data.men)]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge on Specific Columns\n",
    "\n",
    "In the previous example, the merge function \"knew\" how to combine tables based on the columns that were the same between two tables. For instance, products and orders both had a column called product_id. This won't always be true when we want to perform a merge.\n",
    "\n",
    "Generally, the products and customers DataFrames would not have the columns product_id or customer_id. Instead, they would both be called id and it would be implied that the id was the product_id for the products table and customer_id for the customers table. They would look like this:\n",
    "\n",
    "### Customers\n",
    "\n",
    "<table>\n",
    "    <tr><th>id</th><th>customer_name</th><th>address</th><th>phone_number</th></tr>\n",
    "    <tr><th>1</th><td>John Smith</td><td>123 Main St.</td><td>212-123-4567</td></tr>\n",
    "    <tr><th>2</th><td>Jane Doe</td><td>456 Park Ave.</td><td>949-867-5309</td></tr>\n",
    "    <tr><th>3</th><td>Joe Schmo</td><td>798 Broadway</td><td>112-358-1321</td></tr>\n",
    "</table>\n",
    "\n",
    "### Products\n",
    "\n",
    "<table>\n",
    "    <tr><th>id</th><th>description</th><th>price</th></tr>\n",
    "    <tr><td>1</td><td>thing-a-ma-jig</td><td>5</td></tr>\n",
    "    <tr><td>2</td><td>whatcha-ma-call-it</td><td>10</td></tr>\n",
    "    <tr><td>3</td><td>doo-hickey</td><td>7</td></tr>\n",
    "    <tr><td>4</td><td>gizmo</td><td>3</td></tr>\n",
    "</table>\n",
    "\n",
    "## How would this affect our merges?\n",
    "\n",
    "Because the id columns would mean something different in each table, our default merges would be wrong.\n",
    "\n",
    "One way that we could address this problem is to use .rename to rename the columns for our merges. In the example below, we will rename the column id to customer_id, so that orders and customers have a common column for the merge.\n",
    "\n",
    "`pd.merge(orders,customers.rename(columns={'id': 'customer_id'}))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  customer_id  product_id  quantity   timestamp  \\\n",
      "0         1            2           3         1  2017-01-01   \n",
      "1         5            3           3         3  2017-02-01   \n",
      "2         2            2           2         3  2017-01-01   \n",
      "3         4            3           2         2  2017-02-01   \n",
      "4         3            3           1         1  2017-01-01   \n",
      "5         7            1           1         1  2017-02-02   \n",
      "6         6            1           4         2  2017-03-01   \n",
      "7         8            1           4         1  2017-02-02   \n",
      "\n",
      "          description  price  \n",
      "0          doo-hickey      7  \n",
      "1          doo-hickey      7  \n",
      "2  whatcha-ma-call-it     10  \n",
      "3  whatcha-ma-call-it     10  \n",
      "4      thing-a-ma-jig      5  \n",
      "5      thing-a-ma-jig      5  \n",
      "6               gizmo      3  \n",
      "7               gizmo      3  \n"
     ]
    }
   ],
   "source": [
    "orders_products = pd.merge(orders,products.rename(columns={'id':'product_id'}))\n",
    "print(orders_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise, we learned how to use rename to merge two DataFrames whose columns don't match.\n",
    "\n",
    "If we don't want to do that, we have another option. We could use the keywords left_on and right_on to specify which columns we want to perform the merge on. In the example below, the \"left\" table is the one that comes first (orders), and the \"right\" table is the one that comes second (customers). This syntax says that we should match the customer_id from orders to the id in customers.\n",
    "\n",
    "`pd.merge(orders,customers,left_on='customer_id',right_on='id')`\n",
    "\n",
    "If we use this syntax, we'll end up with two columns called id, one from the first table and one from the second. Pandas won't let you have two columns with the same name, so it will change them to id_x and id_y.\n",
    "\n",
    "It will look like this: \n",
    "\n",
    "<table>\n",
    "    <tr><th>id_x</th><th>customer_id</th><th>product_id</th><th>quantity</th><th>timestamp</th><th>id_y</th><th>customer_name</th><th>address</th><th>phone_number</th></tr>\n",
    "    <tr><td>1</td><td>2</td><td>3</td><td>1</td><td>2017-01-01 00:00:00</td><td>2</td><td>Jane Doe</td><td>456 Park Ave.</td><td>949-867-5309</td></tr>\n",
    "    <tr><td>2</td><td>2</td><td>2</td><td>3</td><td>2017-01-01 00:00:00</td><td>2</td><td>Jane Doe</td><td>456 Park Ave.</td><td>949-867-5309</td></tr>\n",
    "    <tr><td>3</td><td>3</td><td>1</td><td>1</td><td>2017-01-01 00:00:00</td><td>3</td><td>Joe Schmo</td><td>798 Broadway</td><td>112-358-1321</td></tr>\n",
    "    <tr><td>4</td><td>3</td><td>2</td><td>2</td><td>2017-02-01 00:00:00</td><td>3</td><td>Joe Schmo</td><td>798 Broadway</td><td>112-358-1321</td></tr>\n",
    "    <tr><td>5</td><td>3</td><td>3</td><td>3</td><td>2017-02-01 00:00:00</td><td>3</td><td>Joe Schmo</td><td>798 Broadway</td><td>112-358-1321</td></tr>\n",
    "    <tr><td>6</td><td>1</td><td>4</td><td>2</td><td>2017-03-01 00:00:00</td><td>1</td><td>John Smith</td><td>123 Main St.</td><td>212-123-4567</td></tr>\n",
    "    <tr><td>7</td><td>1</td><td>1</td><td>1</td><td>2017-02-02 00:00:00</td><td>1</td><td>John Smith</td><td>123 Main St.</td><td>212-123-4567</td></tr>\n",
    "    <tr><td>8</td><td>1</td><td>4</td><td>1</td><td>2017-02-02 00:00:00</td><td>1</td><td>John Smith</td><td>123 Main St.</td><td>212-123-4567</td></tr>\n",
    "</table>\n",
    "\n",
    "The new column names id_x and id_y aren't very helpful for us when we read the table. We can help make them more useful by using the keyword suffixes. We can provide a list of suffixes to use instead of \"_x\" and \"_y\".\n",
    "\n",
    "For example, we could use the following code to make the suffixes reflect the table names:\n",
    "\n",
    "`pd.merge(orders,customers,left_on='customer_id',right_on='id',suffixes=['_order', '_customer'])`\n",
    "\n",
    "The resulting table would look like this: \n",
    "\n",
    "<table>\n",
    "    <tr><th>id_order</th><th>customer_id</th><th>product_id</th><th>quantity</th><th>timestamp</th><th>id_customer</th><th>customer_name</th><th>address</th><th>phone_number</th></tr>\n",
    "    <tr><td>1</td><td>2</td><td>3</td><td>1</td><td>2017-01-01 00:00:00</td><td>2</td><td>Jane Doe</td><td>456 Park Ave.</td><td>949-867-5309</td></tr>\n",
    "    <tr><td>2</td><td>2</td><td>2</td><td>3</td><td>2017-01-01 00:00:00</td><td>2</td><td>Jane Doe</td><td>456 Park Ave.</td><td>949-867-5309</td></tr>\n",
    "    <tr><td>3</td><td>3</td><td>1</td><td>1</td><td>2017-01-01 00:00:00</td><td>3</td><td>Joe Schmo</td><td>798 Broadway</td><td>112-358-1321</td></tr>\n",
    "    <tr><td>4</td><td>3</td><td>2</td><td>2</td><td>2017-02-01 00:00:00</td><td>3</td><td>Joe Schmo</td><td>798 Broadway</td><td>112-358-1321</td></tr>\n",
    "    <tr><td>5</td><td>3</td><td>3</td><td>3</td><td>2017-02-01 00:00:00</td><td>3</td><td>Joe Schmo</td><td>798 Broadway</td><td>112-358-1321</td></tr>\n",
    "    <tr><td>6</td><td>1</td><td>4</td><td>2</td><td>2017-03-01 00:00:00</td><td>1</td><td>John Smith</td><td>123 Main St.</td><td>212-123-4567</td></tr>\n",
    "    <tr><td>7</td><td>1</td><td>1</td><td>1</td><td>2017-02-02 00:00:00</td><td>1</td><td>John Smith</td><td>123 Main St.</td><td>212-123-4567</td></tr>\n",
    "    <tr><td>8</td><td>1</td><td>4</td><td>1</td><td>2017-02-02 00:00:00</td><td>1</td><td>John Smith</td><td>123 Main St.</td><td>212-123-4567</td></tr>\n",
    "</table>\n",
    "\n",
    "Merge orders and products using left_on and right_on. Use the suffixes _orders and _products. Save your results to the variable orders_products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'order_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\brocd8s\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'order_id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0bf934bfc22f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0morders_products\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproducts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft_on\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'product_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright_on\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'order_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msuffixes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'_orders'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_products'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morders_products\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brocd8s\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     55\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                          validate=validate)\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brocd8s\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    563\u001b[0m         (self.left_join_keys,\n\u001b[0;32m    564\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brocd8s\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    822\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brocd8s\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brocd8s\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brocd8s\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brocd8s\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\brocd8s\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'order_id'"
     ]
    }
   ],
   "source": [
    "orders_products = pd.merge(orders,products,left_on = 'product_id',right_on = 'order_id',suffixes = ['_orders', '_products'])\n",
    "print(orders_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer Merge\n",
    "\n",
    "In the previous exercise, we saw that when we merge two DataFrames whose rows don't match perfectly, we lose the unmatched rows.\n",
    "\n",
    "This type of merge (where we only include matching rows) is called an inner merge. There are other types of merges that we can use when we want to keep information from the unmatched rows.\n",
    "\n",
    "Suppose that two companies, Company A and Company B have just merged. They each have a list of customers, but they keep slightly different data. Company A has each customer's name and email. Company B has each customer's name and phone number. They have some customers in common, but some are different.\n",
    "\n",
    "* company_a\n",
    "\n",
    "<table>\n",
    "    <tr><th>name</th><th>email</th></tr>\n",
    "    <tr><td>Sally Sparrow</td><td>sally.sparrow@gmail.com</td></tr>\n",
    "    <tr><td>Peter Grant</td><td>pgrant@yahoo.com</td></tr>\n",
    "    <tr><td>Leslie May</td><td>leslie_may@gmail.com</td></tr>\n",
    "</table>\n",
    "\n",
    "* company_b\n",
    "\n",
    "<table>\n",
    "    <tr><th>name</th><th>phone</th></tr>\n",
    "    <tr><td>Peter Grant</td><td>626-987-6543</td></tr>\n",
    "    <tr><td>Leslie May</td><td>626-987-6453</td></tr>\n",
    "    <tr><td>Aaron Burr</td><td>303-456-7891</td></tr>\n",
    "</table>\n",
    "\n",
    "If we wanted to combine the data from both companies without losing the customers who are missing from one of the tables, we could use an Outer Join. An Outer Join would include all rows from both tables, even if they don't match. Any missing values are filled in with None or nan (which stands for \"Not a Number\").\n",
    "\n",
    "`pd.merge(company_a, company_b, how='outer')`\n",
    "\n",
    "The resulting table would look like this:\n",
    "\n",
    "<table>\n",
    "    <tr><th>name</th><th>email</th><th>phone</th></tr>\n",
    "    <tr><td>Sally Sparrow</td><td>sally.sparrow@gmail.com</td><td>nan</td></tr>\n",
    "    <tr><td>Peter Grant</td><td>pgrant@yahoo.com</td><td>212-345-6789</td></tr>\n",
    "    <tr><td>Leslie May</td><td>leslie_may@gmail.com</td><td>626-987-6543</td></tr>\n",
    "    <tr><td>Aaron Burr</td><td>nan</td><td>303-456-7891</td></tr>\n",
    "</table>\n",
    "\n",
    "There are two hardware stores in town: Store A and Store B. Store A's inventory is in DataFrame store_a and Store B's inventory is in DataFrame store_b. They have decided to merge into one big Super Store!\n",
    "\n",
    "Combine the inventories of Store A and Store B using an outer merge. Save the results to the variable store_a_b_outer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             item  store_a_inventory  store_b_inventory\n",
      "0          hammer               12.0                6.0\n",
      "1     screwdriver               15.0                NaN\n",
      "2           nails              200.0              250.0\n",
      "3          screws              350.0                NaN\n",
      "4             saw                6.0                6.0\n",
      "5       duct tape              150.0              150.0\n",
      "6          wrench               12.0                NaN\n",
      "7        pvc pipe               54.0               54.0\n",
      "8            rake                NaN               10.0\n",
      "9          shovel                NaN               15.0\n",
      "10  wooden dowels                NaN              192.0\n"
     ]
    }
   ],
   "source": [
    "store_a = pd.read_csv('store_a.csv')\n",
    "store_b = pd.read_csv('store_b.csv')\n",
    "\n",
    "store_a_b_outer = pd.merge(store_a, store_b, how='outer')\n",
    "print(store_a_b_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Merge\n",
    "\n",
    "Let's return to the merge of Company A and Company B.\n",
    "\n",
    "Suppose we want to identify which customers are missing phone information. We would want a list of all customers who have email, but don't have phone.\n",
    "\n",
    "We could get this by performing a Left Merge. A Left Merge includes all rows from the first (left) table, but only rows from the second (right) table that match the first table.\n",
    "\n",
    "For this command, the order of the arguments matters. If the first DataFrame is company_a and we do a left join, we'll only end up with rows that appear in company_a.\n",
    "\n",
    "By listing company_a first, we get all customers from Company A, and only customers from Company B who are also customers of Company B.\n",
    "\n",
    "`pd.merge(company_a, company_b, how='left')`\n",
    "\n",
    "The result would look like this:\n",
    "\n",
    "<table>\n",
    "    <tr><th>name</th><th>email</th><th>phone</th></tr>\n",
    "    <tr><td>Sally Sparrow</td><td>sally.sparrow@gmail.com</td><td>None</td></tr>\n",
    "    <tr><td>Peter Grant</td><td>pgrant@yahoo.com</td><td>212-345-6789</td></tr>\n",
    "    <tr><td>Leslie May</td><td>leslie_may@gmail.com</td><td>626-987-6543</td></tr>\n",
    "</table>\n",
    "\n",
    "If we reverse the order of company_b and company_a, we'll only get rows that appear in company_b:\n",
    "\n",
    "`pd.merge(company_b, company_a, how='left')`\n",
    "\n",
    "<table>\n",
    "    <tr><th>name</th><th>email</th><th>phone</th></tr>\n",
    "    <tr><td>Peter Grant</td><td>pgrant@yahoo.com</td><td>212-345-6789</td></tr>\n",
    "    <tr><td>Leslie May</td><td>leslie_may@gmail.com</td><td>626-987-6543</td></tr>\n",
    "    <tr><td>Aaron Burr</td><td>None</td><td>303-456-7891</td></tr>\n",
    "</table>\n",
    "\n",
    "Let's return to the two hardware stores, Store A and Store B. They're not quite sure if they want to merge into a big Super Store just yet.\n",
    "\n",
    "Store A wants to find out what products they carry that Store B does not carry. Using a left merge, combine store_a to store_b and save the results to store_a_b_left.\n",
    "\n",
    "The items with null in store_b_inventory are carried by Store A, but not Store B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          item  store_a_inventory  store_b_inventory\n",
      "0       hammer                 12                6.0\n",
      "1  screwdriver                 15                NaN\n",
      "2        nails                200              250.0\n",
      "3       screws                350                NaN\n",
      "4          saw                  6                6.0\n",
      "5    duct tape                150              150.0\n",
      "6       wrench                 12                NaN\n",
      "7     pvc pipe                 54               54.0 \n",
      "\n",
      "            item  store_b_inventory  store_a_inventory\n",
      "0         hammer                  6               12.0\n",
      "1          nails                250              200.0\n",
      "2            saw                  6                6.0\n",
      "3      duct tape                150              150.0\n",
      "4       pvc pipe                 54               54.0\n",
      "5           rake                 10                NaN\n",
      "6         shovel                 15                NaN\n",
      "7  wooden dowels                192                NaN\n"
     ]
    }
   ],
   "source": [
    "store_a_b_left = pd.merge(store_a, store_b, how='left')\n",
    "store_b_a_left = pd.merge(store_b, store_a, how='left')\n",
    "\n",
    "print(store_a_b_left,'\\n')\n",
    "print(store_b_a_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate DataFrames\n",
    "\n",
    "Sometimes, a dataset is broken into multiple tables. For instance, data is often split into multiple CSV files so that each download is smaller.\n",
    "\n",
    "When we need to reconstruct a single DataFrame from multiple smaller DataFrames, we can use the method pd.concat([df1, df2, df2, ...]). This method only works if all of the columns are the same in all of the DataFrames.\n",
    "\n",
    "For instance, suppose that we have two DataFrames:\n",
    "\n",
    "* df1\n",
    "\n",
    "<table>\n",
    "    <tr><th>name</th><th>email</th></tr>\n",
    "    <tr><td>Katja Obinger</td><td>k.obinger@gmail.com</td></tr>\n",
    "    <tr><td>Alison Hendrix</td><td>alisonH@yahoo.com</td></tr>\n",
    "    <tr><td>Cosima Niehaus</td><td>cosi.niehaus@gmail.com</td></tr>\n",
    "    <tr><td>Rachel Duncan</td><td>rachelduncan@hotmail.com</td></tr>\n",
    "</table>\n",
    "\n",
    "* df2\n",
    "\n",
    "<table>\n",
    "    <tr><th>name</th><th>email</th></tr>\n",
    "    <tr><td>Jean Gray</td><td>jgray@netscape.net</td></tr>\n",
    "    <tr><td>Scott Summers</td><td>ssummers@gmail.com</td></tr>\n",
    "    <tr><td>Kitty Pryde</td><td>kitkat@gmail.com</td></tr>\n",
    "    <tr><td>Charles Xavier</td><td>cxavier@hotmail.com</td></tr>\n",
    "</table>\n",
    "\n",
    "If we want to combine these two DataFrames, we can use the following command:\n",
    "\n",
    "`pd.concat([df1, df2])`\n",
    "\n",
    "That would result in the following DataFrame:\n",
    "\n",
    "<table>\n",
    "    <tr><th>name</th><th>email</th></tr>\n",
    "    <tr><td>Katja Obinger</td><td>k.obinger@gmail.com</td></tr>\n",
    "    <tr><td>Alison Hendrix</td><td>alisonH@yahoo.com</td></tr>\n",
    "    <tr><td>Cosima Niehaus</td><td>cosi.niehaus@gmail.com</td></tr>\n",
    "    <tr><td>Rachel Duncan</td><td>rachelduncan@hotmail.com</td></tr>\n",
    "    <tr><td>Jean Gray</td><td>jgray@netscape.net</td></tr>\n",
    "    <tr><td>Scott Summers</td><td>ssummers@gmail.com</td></tr>\n",
    "    <tr><td>Kitty Pryde</td><td>kitkat@gmail.com</td></tr>\n",
    "    <tr><td>Charles Xavier</td><td>cxavier@hotmail.com</td></tr>\n",
    "</table>\n",
    "\n",
    "An ice cream parlor and a bakery have decided to merge.\n",
    "\n",
    "The bakery's menu is stored in the DataFrame bakery, and the ice cream parlor's menu is stored in DataFrame ice_cream.\n",
    "\n",
    "Create their new menu by concatenating the two DataFrames into a DataFrame called menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              item  price\n",
      "0                           cookie   2.50\n",
      "1                          brownie   3.50\n",
      "2                    slice of cake   4.75\n",
      "3              slice of cheesecake   4.75\n",
      "4                     slice of pie   5.00\n",
      "0     scoop of chocolate ice cream   3.00\n",
      "1       scoop of vanilla ice cream   2.95\n",
      "2    scoop of strawberry ice cream   3.05\n",
      "3  scoop of cookie dough ice cream   3.25\n"
     ]
    }
   ],
   "source": [
    "bakery = pd.read_csv('bakery.csv')\n",
    "ice_cream = pd.read_csv('ice_cream.csv')\n",
    "\n",
    "menu = pd.concat([bakery, ice_cream])\n",
    "print(menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool T-Shirts Inc. just created a website for ordering their products. They want you to analyze two datasets for them:\n",
    "\n",
    "* visits contains information on all visits to their landing page\n",
    "* checkouts contains all users who began to checkout on their website\n",
    "\n",
    "Use print to inspect each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                user_id          visit_time\n",
      "0  319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:16:00\n",
      "1  7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:37:00\n",
      "2  0b061e73-f709-42fa-8d1a-5f68176ff154 2017-04-12 19:32:00\n",
      "3  9133d6f0-e68b-4c8d-bafd-ff2825e8dafe 2017-08-18 04:32:00\n",
      "4  08d13edb-071c-4cfb-9ee4-8f377d0e932a 2017-07-08 06:24:00 \n",
      "\n",
      "                                user_id       checkout_time\n",
      "0  fe90a9f4-960a-4a0d-9160-e562adb79365 2017-11-09 09:25:00\n",
      "1  1a35b7eb-f603-407d-91be-a2c3304066fd 2017-08-15 21:25:00\n",
      "2  e2c24ee0-7fdf-4400-abde-b36378fe5ce6 2017-07-04 15:39:00\n",
      "3  10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:07:00\n",
      "4  f028e9dd-77d0-4002-83f6-372a4837fda6 2017-10-27 08:57:00\n"
     ]
    }
   ],
   "source": [
    "visits = pd.read_csv('visits.csv',\n",
    "                        parse_dates=[1])\n",
    "print(visits.head(),'\\n')\n",
    "checkouts = pd.read_csv('checkouts.csv',\n",
    "                        parse_dates=[1])\n",
    "print(checkouts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know the amount of time from a user's initial visit to the website to when they start to check out.\n",
    "\n",
    "Use merge to combine visits and checkouts and save it to the variable v_to_c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_to_c = pd.merge(visits, checkouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the time between visiting and checking out, define a column of v_to_c called time by pasting the following code into script.py:\n",
    "\n",
    "```\n",
    "v_to_c['time'] = v_to_c.checkout_time - v_to_c.visit_time\n",
    "print(v_to_c)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 user_id          visit_time  \\\n",
      "0   319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:16:00   \n",
      "1   319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:16:00   \n",
      "2   7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:37:00   \n",
      "3   7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:37:00   \n",
      "4   08d13edb-071c-4cfb-9ee4-8f377d0e932a 2017-07-08 06:24:00   \n",
      "5   c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:19:00   \n",
      "6   c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:19:00   \n",
      "7   c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:19:00   \n",
      "8   c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:19:00   \n",
      "9   f028e9dd-77d0-4002-83f6-372a4837fda6 2017-10-27 08:46:00   \n",
      "10  746631d2-35d5-441e-a21b-e5f39442f981 2017-06-19 23:34:00   \n",
      "11  746631d2-35d5-441e-a21b-e5f39442f981 2017-06-19 23:34:00   \n",
      "12  a0fc94a2-4a80-4a33-994b-75783066ac62 2017-05-11 13:07:00   \n",
      "13  e2c24ee0-7fdf-4400-abde-b36378fe5ce6 2017-07-04 15:33:00   \n",
      "14  e6c7ecb9-4710-4cbd-ad02-c43971ebbe7f 2017-09-27 16:10:00   \n",
      "15  fe07fc99-3943-4000-99a5-422957a42ea1 2017-10-07 15:08:00   \n",
      "16  ad7a3539-ad48-4b0e-bcc3-8c0dff722908 2017-06-13 08:28:00   \n",
      "17  ad7a3539-ad48-4b0e-bcc3-8c0dff722908 2017-06-13 08:28:00   \n",
      "18  10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:01:00   \n",
      "19  10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:01:00   \n",
      "20  9e744240-18e8-4da0-9e05-89b116245c15 2017-11-02 20:01:00   \n",
      "21  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:15:00   \n",
      "22  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:15:00   \n",
      "23  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:15:00   \n",
      "24  f8123e12-d349-4efe-ae65-9494630bee6c 2017-02-22 13:16:00   \n",
      "25  0d798d59-fb81-40ae-8ab1-d4beaffe8715 2017-01-18 17:26:00   \n",
      "26  0d798d59-fb81-40ae-8ab1-d4beaffe8715 2017-01-18 17:26:00   \n",
      "27  f5f90dcf-15a0-432b-9886-9e4b5907a1cc 2017-07-10 11:39:00   \n",
      "28  f5f90dcf-15a0-432b-9886-9e4b5907a1cc 2017-07-10 11:39:00   \n",
      "29  65599d0d-76c2-4ad4-a717-70f12a187f1a 2017-02-15 07:21:00   \n",
      "..                                   ...                 ...   \n",
      "50  cb602c66-1366-467a-8bee-52477310cf42 2017-07-03 06:15:00   \n",
      "51  3a46dc74-4505-4a1a-8dde-4268a7484321 2017-09-17 22:34:00   \n",
      "52  67905d3e-5332-4927-a8ca-7ae9daabc7b8 2017-05-20 03:25:00   \n",
      "53  d2ae16fa-abc6-4755-af80-5010a7c6a103 2017-04-19 03:05:00   \n",
      "54  20fee849-260d-439f-a63c-bc2a0ebb6eb9 2017-01-04 10:43:00   \n",
      "55  010db559-8316-46b5-be65-207cecba63a6 2017-04-05 10:34:00   \n",
      "56  010db559-8316-46b5-be65-207cecba63a6 2017-04-05 10:34:00   \n",
      "57  287c13f5-9a9a-4c3a-9f0d-13853e8aad9b 2017-03-27 23:30:00   \n",
      "58  280c9dcb-2c9f-4f33-ada1-92196c0b1d37 2017-04-10 14:55:00   \n",
      "59  f74519df-e961-4841-acdb-2d47da194aba 2017-11-17 21:05:00   \n",
      "60  7fe800cc-46e8-427c-a7af-f27198d305a1 2017-01-18 12:50:00   \n",
      "61  7fe800cc-46e8-427c-a7af-f27198d305a1 2017-01-18 12:50:00   \n",
      "62  fe90a9f4-960a-4a0d-9160-e562adb79365 2017-11-09 09:04:00   \n",
      "63  52b650a4-f315-4947-804f-19df5f971d85 2017-07-05 21:15:00   \n",
      "64  9812465d-fded-43c4-8685-3e96446f6cc7 2017-07-19 17:02:00   \n",
      "65  f41d2868-515d-49a2-b48c-e4f33e5d9b69 2017-09-07 16:40:00   \n",
      "66  43db76fc-d522-450d-a371-ef2a683d5bfd 2017-03-26 21:11:00   \n",
      "67  1a35b7eb-f603-407d-91be-a2c3304066fd 2017-08-15 21:09:00   \n",
      "68  b7953447-00a8-42be-99d2-b511f4e9c12b 2017-04-24 10:13:00   \n",
      "69  b7953447-00a8-42be-99d2-b511f4e9c12b 2017-04-24 10:13:00   \n",
      "70  2f1e93f2-4d40-45e1-8371-7c65660f6bf9 2017-10-25 19:42:00   \n",
      "71  2f1e93f2-4d40-45e1-8371-7c65660f6bf9 2017-10-25 19:42:00   \n",
      "72  2d747792-db15-4549-abb8-ef352884b3db 2017-01-16 18:10:00   \n",
      "73  2d747792-db15-4549-abb8-ef352884b3db 2017-01-16 18:10:00   \n",
      "74  2d747792-db15-4549-abb8-ef352884b3db 2017-01-16 18:10:00   \n",
      "75  23a8d1be-3f5c-4b59-aed7-c7f19c51612b 2017-08-11 13:49:00   \n",
      "76  8d9ac96c-16be-418e-8df4-1a6202d0b36e 2017-10-07 10:23:00   \n",
      "77  8d9ac96c-16be-418e-8df4-1a6202d0b36e 2017-10-07 10:23:00   \n",
      "78  5679519b-a901-4970-8656-dbf60ffb618d 2017-07-20 04:23:00   \n",
      "79  fff8f87a-e4a2-4f2c-b3d4-93a4ece95c4f 2017-06-06 23:42:00   \n",
      "\n",
      "         checkout_time     time  \n",
      "0  2017-02-21 07:27:00 00:11:00  \n",
      "1  2017-02-21 07:40:00 00:24:00  \n",
      "2  2017-05-16 08:49:00 00:12:00  \n",
      "3  2017-05-16 08:55:00 00:18:00  \n",
      "4  2017-07-08 06:32:00 00:08:00  \n",
      "5  2017-07-09 14:26:00 00:07:00  \n",
      "6  2017-07-09 14:24:00 00:05:00  \n",
      "7  2017-07-09 14:39:00 00:20:00  \n",
      "8  2017-07-09 14:42:00 00:23:00  \n",
      "9  2017-10-27 08:57:00 00:11:00  \n",
      "10 2017-06-19 23:52:00 00:18:00  \n",
      "11 2017-06-19 23:47:00 00:13:00  \n",
      "12 2017-05-11 13:31:00 00:24:00  \n",
      "13 2017-07-04 15:39:00 00:06:00  \n",
      "14 2017-09-27 16:30:00 00:20:00  \n",
      "15 2017-10-07 15:25:00 00:17:00  \n",
      "16 2017-06-13 08:40:00 00:12:00  \n",
      "17 2017-06-13 08:50:00 00:22:00  \n",
      "18 2017-08-09 21:07:00 00:06:00  \n",
      "19 2017-08-09 21:04:00 00:03:00  \n",
      "20 2017-11-02 20:04:00 00:03:00  \n",
      "21 2017-05-03 13:19:00 00:04:00  \n",
      "22 2017-05-03 13:15:00 00:00:00  \n",
      "23 2017-05-03 13:25:00 00:10:00  \n",
      "24 2017-02-22 13:44:00 00:28:00  \n",
      "25 2017-01-18 17:48:00 00:22:00  \n",
      "26 2017-01-18 17:38:00 00:12:00  \n",
      "27 2017-07-10 12:08:00 00:29:00  \n",
      "28 2017-07-10 12:03:00 00:24:00  \n",
      "29 2017-02-15 07:27:00 00:06:00  \n",
      "..                 ...      ...  \n",
      "50 2017-07-03 06:37:00 00:22:00  \n",
      "51 2017-09-17 23:00:00 00:26:00  \n",
      "52 2017-05-20 03:44:00 00:19:00  \n",
      "53 2017-04-19 03:21:00 00:16:00  \n",
      "54 2017-01-04 10:58:00 00:15:00  \n",
      "55 2017-04-05 10:42:00 00:08:00  \n",
      "56 2017-04-05 10:35:00 00:01:00  \n",
      "57 2017-03-27 23:44:00 00:14:00  \n",
      "58 2017-04-10 14:57:00 00:02:00  \n",
      "59 2017-11-17 21:08:00 00:03:00  \n",
      "60 2017-01-18 13:09:00 00:19:00  \n",
      "61 2017-01-18 13:14:00 00:24:00  \n",
      "62 2017-11-09 09:25:00 00:21:00  \n",
      "63 2017-07-05 21:42:00 00:27:00  \n",
      "64 2017-07-19 17:15:00 00:13:00  \n",
      "65 2017-09-07 16:47:00 00:07:00  \n",
      "66 2017-03-26 21:31:00 00:20:00  \n",
      "67 2017-08-15 21:25:00 00:16:00  \n",
      "68 2017-04-24 10:20:00 00:07:00  \n",
      "69 2017-04-24 10:26:00 00:13:00  \n",
      "70 2017-10-25 19:49:00 00:07:00  \n",
      "71 2017-10-25 20:05:00 00:23:00  \n",
      "72 2017-01-16 18:30:00 00:20:00  \n",
      "73 2017-01-16 18:10:00 00:00:00  \n",
      "74 2017-01-16 18:39:00 00:29:00  \n",
      "75 2017-08-11 14:11:00 00:22:00  \n",
      "76 2017-10-07 10:24:00 00:01:00  \n",
      "77 2017-10-07 10:52:00 00:29:00  \n",
      "78 2017-07-20 04:24:00 00:01:00  \n",
      "79 2017-06-07 00:11:00 00:29:00  \n",
      "\n",
      "[80 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "v_to_c['time'] = v_to_c.checkout_time - v_to_c.visit_time\n",
    "print(v_to_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the average time to checkout, paste the following code into script.py:\n",
    "\n",
    "`print(v_to_c.time.mean())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 00:15:24.750000\n"
     ]
    }
   ],
   "source": [
    "print(v_to_c.time.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
